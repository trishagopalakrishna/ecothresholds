```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, include=FALSE}
library(RColorBrewer)
library(lattice)
library(tidyverse)
library(here)
library(tictoc)


library(ggplot2)
library(ggpubr)
library(sf)
library(terra)

library(tmap)
library(tmaptools)
library(RColorBrewer)
library(viridis)

terraOptions(memfrac=0.5, tempdir = here("Scratch"), progress=10)
```

##Introduction
In this script I explore the time series decomposition and then regression analyses of the decomposed components of the monthly anisoEVI values. 

##Data input
```{r}
masked_evi_list <- list.files(path = here("Outputs", "Indices", "Masked_anisoEVI"), pattern='.tif$', all.files=TRUE, full.names=TRUE)
masked_evi_list<-gtools::mixedsort(masked_evi_list) #arranging rasters chronologically
masked_evi_raster_list<-lapply(masked_evi_list, rast)
aniso_evi<- rast(masked_evi_raster_list)

##changing band names to year and months
colnames<- list()
tic(); for ( i in 1:length(masked_evi_list)){
  x<- paste0(strsplit(strsplit(masked_evi_list[i], "/")[[1]][10], "_")[[1]][1], "_", strsplit(strsplit(masked_evi_list[i], "/")[[1]][10], "_")[[1]][2])
  colnames[[i]] <-x
}; toc()
colnames<- unlist(colnames)
names(aniso_evi)<- colnames

d_trans<- st_read(here ("Data", "Cattelanetal_Clustering", "cerrado_climate_zones.shp"))
d_trans<- st_transform(d_trans, crs = 4326)

buffer<- st_read(here ("Data", "Cattelanetal_Clustering", "buffer_climate_zones.shp"))

```

################################## UNDERSTANDING SEASONAL CYCLE INCLUDING DECOMPOSITION (using BFAST and BFASTLite) & DIFFERENCING

##Decomposition analyses using Bfast
Marina and I discussed decomposing the trend into seasonal and long term trend using the BFAST (Breaks For Additive Seasonal and Trend) algorithm (Verbesselt et al., 2010). I read the Jong et al., 2012 GCB paper about global greening and browning and found it pretty solid. So here I try it out. There is a R package and here I try to better understand it. https://bfast.r-forge.r-project.org/

Also see this post comparing bfast and Rbeast- https://stackoverflow.com/questions/52708697/detect-changes-in-the-seasonal-component-using-bfast. First trying bfast 

#1. First, I select 50 random pixels (points) across the buffered climate zones i.e only one big area across the entire buffer

```{r}
#install.packages(c("Rbeast", "bfast"))

buffer
random_points<-terra::spatSample(vect(buffer), size = 350, method = "random") #350 random points to get good coverage within the actual cerrado boundary
plot(random_points)

random_points_cerrado<- terra::crop(random_points, vect(d_trans))
random_points_cerrado<- terra::mask(random_points_cerrado, vect(d_trans)) #197 points out of 350 total points


#Selecting a couple of random pixels (50) in each of the three zones 
r<- aniso_evi[[251]] #take last raster of anisoevi stack
empty<- rast(ext(r), resolution = res(r))
crs(empty) <- crs(r)
values(empty)<-1




empty_zones<- list()
tic(); for (i in 1:length(d_trans)){
  geom<- d_trans$geometry[[i]]
  rsample<- terra::crop(empty, vect(geom))
  rsample<- terra::mask (rsample, vect(geom))
  empty_zones[[i]]<- rsample
}; toc()

random_points_zones<- list()
tic(); for (i in 1:length(empty_zones)){
  x<- terra::spatSample(empty_zones[[i]], size = 50, method = "random", replace = FALSE, as.points= TRUE, na.rm=T)
  random_points_zones[[i]]<- x
}; toc()

all_random_points<- do.call(rbind,random_points_zones)
all_random_points<- st_as_sf(all_random_points) %>% mutate(PointID= 1:dim(all_random_points)[1])
all_random_points<- all_random_points %>% st_join(d_trans)




all_random_points$region #there are 2 points with NA region. I checked in QGIS. These two points are on the border of the southern region
all_random_points<- all_random_points %>% mutate(region = ifelse(is.na(region), "Southern", region))
st_write(all_random_points, here("Outputs","bfastTrials", "randompoints_generated_bfast_trial.shp"))
remove(r, empty, empty_zones, random_points_zones)

neighbors<- st_read(here("Data", "Admin", "brazil_neighbors.shp"))

map_extent<- st_bbox(c(xmin=-77.59, xmax=-31.09,
                       ymin=6.42, ymax=-33.49), crs=4326) %>% st_as_sfc()

random_points_map<-tm_shape(neighbors, bbox = map_extent)+ tm_borders()+
  tm_shape(d_trans)+ tm_fill()+
  tm_shape( st_as_sf(all_random_points))+ tm_dots()+
              tm_layout (legend.position = c("left", "bottom"))+
              tm_facets(nrow=1, ncol=1, free.coords = FALSE)
tmap_save(random_points_map, here("Outputs", "bfastTrials","random_points_map.png") )
remove(neighbors, map_extent, random_points_map)
```
(DO NOT RUN ABOVE, ALREADY WRITTEN OUT)

#2. Trial BFAST
```{r}
library(bfast)
#Sampling anisoEVI through time in the random pixels
all_random_points<- st_read(here("Outputs", "BFAST_Decomposition", "randompoints_generated_bfast_trial.shp"))
tic(); anisoEVI_random_points<- terra::extract(aniso_evi, all_random_points, method= "simple", xy=TRUE); toc()

# Trial BFAST across above points
#The main parameter to be input is h. h is calculated as the minimal segment size between potentially detected breaks in the trend model given as fraction relative to the sample size (i.e. the minimal number of observations in each segment divided by the total length of the timeseries). 
#So I understand in terms of number of points of season switches i.e 2 a year which is 2*21 across the entire time-series 
#which means h= (2*21)/261 (because h is a proportion). So trying this out for all random points below


for (i in 1:length(all_random_points$PointID)){
  print (paste0("Point number ", i))
  one_point<- anisoEVI_random_points[i,] %>% dplyr::select(-c("ID", "x", "y"))
  print ("Creating time series object")
  x.ts <- ts(unlist(one_point), frequency = 12) 
  tic(); trial<- bfast::bfast(x.ts, h=(2*21)/261, season = "harmonic", max.iter = 20); toc()
  print ("Completed bfast decomposition")
  dev.off()
  jpeg(here("Outputs", "BFAST_Decomposition", paste0("decomposition_point_",all_random_points$PointID[i], ".jpg")))
  plot(trial)
  dev.off()
  print ("Check jpeg file on disk")
}

one_point<- anisoEVI_random_points[1,] %>% dplyr::select(-c("ID", "x", "y"))
nbr.NA <- sum(!is.na(as.vector(one_point))) #Of 261 month steps, there are 257 with values i.e. 4 NA values
x.ts <- ts(unlist(one_point), frequency = 12) 
tic(); trial<- bfast::bfast(x.ts, h=(2*21)/261, season = "harmonic", max.iter = 20); toc()
remove(nbr.NA)
#Consider all points


```

#3. Trial BFASTLite
The main thing about bfast lite I do not 'like' or do not understand is the 'order' which is a 'harmonic term' and defaults to 3. So I dug deeper and found this article https://lpsa.swarthmore.edu/Fourier/Series/WhyFS.html

So order is basically the harmonic term in a fourier series. And higher the order, the better the approximation to the i.e. sum of average+ season gets closest to original observations. I think higher the term, greater the frequency of the 'season' and shorter the time period within which the season happens. 

I do not include/think about lag, slag and na.action terms for now. For breaks, I let the algorithm automatically determine the optimal number of breaks based on LWZ metric (see bfastlite original paper). I also do not do any STL adjustment so stl="none"

I do a couple of trials below. For the default trial1 I follow the example in the R help for bfastlite. In the example, the break is at the 99th observation (out of 199). And then the example uses strucchangeRcpp::breakpoints with number of breaks specified as 2. I do not understand why. Same with plotting the bfastlite model, the example says breaks=2. I do not know why!  

```{r}
#default order=3
one_point<- anisoEVI_random_points[i,] %>% dplyr::select(-c("ID", "x", "y")) #has only 1 NA in Jan 2003
x.ts <- ts(unlist(one_point), frequency = 12) 
tic(); trial1<- bfast::bfastlite(x.ts, order=3, breaks = "LWZ", lag = NULL, slag = NULL, na.action = na.omit, stl = "none"); toc()
#NA breakpoints
plot(trial1) # blue line is the breakpoint, green line is the fitted model

#default order=7
tic(); trial2<- bfast::bfastlite(x.ts, order=7, breaks = "LWZ", lag = NULL, slag = NULL, na.action = na.omit, stl = "none"); toc()
#Again NA breakpoints

breakpoints<-all_random_points
breakpoints<- breakpoints %>% mutate(ObservationBreakpoint=NA)
tic(); for (i in 135:length(all_random_points$PointID)){
  print (paste0("Point number ", i))
  one_point<- anisoEVI_random_points[i,] %>% dplyr::select(-c("ID", "x", "y"))
  print ("Creating time series object")
  x.ts <- ts(unlist(one_point), frequency = 12) 
  tic(); trial<- bfast::bfastlite(x.ts, order=3, breaks="LWZ", lag=NULL, slag=NULL, na.action= na.omit, stl="none")
  print ("Completed bfastlite analyses")
  breakpoint<- trial$breakpoints$breakpoints
  print (breakpoint)
  breakpoints$ObservationBreakpoint[i]<- breakpoint
  print ("Breakpoint observation number saved")
  dev.new()
  jpeg(here("Outputs", "bfastlite", "Order3", paste0("bfastlite_point_",all_random_points$PointID[i], ".jpg")))
  plot(trial)
  graphics.off()
  print ("Check jpeg file on disk")
}; tic()

#32,50,51,55,60,61,87,97,105,134 points could not be run in analyses because all time series is NA

```



Using the time series anisoEVI data in the random points above, I do some EDA with linear regression to understand how lm() in R treats time series data with NA. 

```{r}
pivot_all_random_points<- pivot_longer(anisoEVI_random_points, cols = 2:262)

no_NA<- pivot_all_random_points %>% dplyr::select(-c(x,y)) %>%
  group_by(ID) %>%  summarise(sum_na = sum(is.na(value)))


absent_NA<- pivot_all_random_points %>% filter(ID==60)
absent_NA<- absent_NA %>% separate_wider_delim(cols = name, delim = "_", names = c("Year","Month"))
absent_NA<- absent_NA %>% mutate(Date= paste0(Year,"_", Month, "_01"))
absent_NA<- absent_NA %>% mutate(Date= lubridate::as_date(Date))
median_3NA<- pivot_all_random_points %>% filter(ID==147)
median_3NA<- median_3NA %>% separate_wider_delim(cols = name, delim = "_", names = c("Year","Month"))
median_3NA<- median_3NA %>% mutate(Date= paste0(Year,"_", Month, "_01"))
median_3NA<- median_3NA %>% mutate(Date= lubridate::as_date(Date))
max_46NA<- pivot_all_random_points %>% filter(ID==7)
max_46NA<- max_46NA %>% separate_wider_delim(cols = name, delim = "_", names = c("Year","Month"))
max_46NA<- max_46NA %>% mutate(Date= paste0(Year,"_", Month, "_01"))
max_46NA<- max_46NA %>% mutate(Date= lubridate::as_date(Date))
#absent_NA<- ts(absent_NA %>% dplyr::select(value),
#                start=c(2000,3), end=c(2021,12), frequency=12)
m = lm(value ~ Date, data = absent_NA)
m1 = lm(value ~ Date, data = median_3NA)
m2 = lm(value ~ Date, data = max_46NA)


```
Basically, lm() can handle NA. It excludes the NA points completely. As NAs increase, the lm still runs but the residual error increases and there is cost of losing degrees of freedom

I was reading about this article about removing seasonal trend from data. It says that 'differencing' is an alternative to decomposition. So I try that in the chunk below
https://atsa-es.github.io/atsa-labs/sec-tslab-differencing-to-remove-a-trend-or-seasonal-effects.html

Specifically look at last sentence under 4.3.1
" In addition, first-differencing a time series at a lag equal to the period will remove a seasonal trend (e.g., set lag = 12 for monthly data)."

```{r}

absentNA_d1_lag12 <- timeseries::diff(absent_NA$value, differences = 1, lag = 12)
median_3NA_d1_lag12 <- timeseries::diff(median_3NA$value, differences = 1, lag = 12)
```






#1- No trend lm
```{r}
no_trend_coef_fun <- function(x) { 
  if (is.na(x[1])){ NA } 
  else { m = lm(x ~ 1); #no trend means, the resulting x will be the mean of all the time series NDVI
  summary(m)$coefficients[1] 
  }}
tic(); no_trend.slope<- terra::app(aniso_evi, no_trend_coef_fun); toc()
plot(no_trend.slope, main="slope")
```



#2- Trying out linear regression curve fitting as per
https://matinbrandt.wordpress.com/2013/11/15/pixel-wise-time-series-trend-anaylsis-with-ndvi-gimms-and-r/
```{r}
time <- 1:nlyr(aniso_evi) 
lm_coef_fun <- function(x) { 
  if (is.na(x[1])){ NA } #if all pixels in the first column (ie same pixels through time) are NA, then result is NA
  else { m = lm(x ~ time); 
  summary(m)$coefficients[2] 
  }}
tic(); evi.slope<- terra::app(aniso_evi, lm_coef_fun); toc()
plot(evi.slope, main="slope")

lm_p_fun<- function(x) { 
  if (is.na(x[1])){ NA } #if all pixels in the first column (ie same pixels through time) are NA, then result is NA
  else { m = lm(x ~ time); 
  summary(m)$coefficients[8] 
  }}
tic(); p <- terra::app(aniso_evi, fun=lm_p_fun); toc()
plot(p, main="p-Value")

m = c(0, 0.05, 1, 0.05, 1, 0) 
rclmat <- matrix(m, ncol=3, byrow=TRUE) #<0.05 is significant, so pixel values=1
tic(); p.mask <- terra::classify(p, rclmat); toc()
mask_fun<- function(x) {
  x[x<1] <- NA; #if pixel =0, then make it NA to mask out non-significant p values
  return(x)
  }
tic(); p.mask.NA<- terra:: app(p.mask, mask_fun); toc()

trend.sig <- terra::mask(evi.slope, p.mask.NA)
plot(trend.sig, main="significant NDVI change")


```

