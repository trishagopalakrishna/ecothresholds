```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE, include=FALSE}
library(RColorBrewer)
library(lattice)
library(tidyverse)
library(here)
library(tictoc)


library(ggplot2)
library(ggpubr)
library(sf)
library(terra)

library(tmap)
library(tmaptools)
library(RColorBrewer)
library(viridis)

terraOptions(memfrac=0.8, tempdir = here("Scratch"), progress=10)
```

#Introduction 
In this script I extract the time series information for pixels of interest, complete the classification and model selection steps. 

#1. Data input and prep
```{r}
read_rasters_function<- function(folder_name){
  raster_list<- list.files(path = here("Outputs", "Indices","Caatinga", folder_name), pattern='.tif$', all.files=TRUE, full.names=TRUE)
  raster_list<- gtools::mixedsort(raster_list)
  masked_raster_list<- lapply(raster_list, rast)
  rast_block<- rast(masked_raster_list)
  rast_block
}

tic(); monthly_EVI<- read_rasters_function("masked_monthlyEVI"); toc()
tic(); monthly_NDVI<- read_rasters_function("masked_monthlyNDVI"); toc()

#scaling
tic(); monthly_EVI<- monthly_EVI/10^5; toc()
tic(); monthly_NDVI<- monthly_NDVI*0.0001; toc() #Scale mentioned on GEE product "Bands" page


#Converting to dataframe
tic(); monthly_EVI_df<- terra::as.data.frame(monthly_EVI, xy=T, cells= T); tic()
#write_rds(monthly_EVI_df, here("Outputs", "Indices", "Caatinga", "scaled_monthly_EVI_df.rds"))
tic(); monthly_NDVI_df<- terra::as.data.frame(monthly_NDVI, xy=T, cells= T); tic()
#write_rds(monthly_NDVI_df, here("Outputs", "Indices", "Caatinga", "scaled_monthly_NDVI_df.rds"))

monthly_EVI_df<- read_rds(here("Outputs", "Indices", "Caatinga", "scaled_monthly_EVI_df.rds"))
monthly_NDVI_df <- read_rds(here("Outputs", "Indices", "Caatinga", "scaled_monthly_NDVI_df.rds"))

#Pivot each dataframe and reorder by month
tic(); pivot_monthly_EVI<- monthly_EVI_df %>% pivot_longer(4:255); toc() 
tic(); pivot_monthly_EVI<- pivot_monthly_EVI %>% separate_wider_delim(cols = name, delim = "_", names = c("Year","Month_Chr"));toc()
tic(); pivot_monthly_EVI<- pivot_monthly_EVI %>% mutate(Month_Num= case_when(Month_Chr=="Jan"~1,
                                                                             Month_Chr=="Feb"~2,
                                                                             Month_Chr=="Mar"~3,
                                                                             Month_Chr=="Apr"~4,
                                                                             Month_Chr=="May"~5,
                                                                             Month_Chr=="Jun"~6,
                                                                             Month_Chr=="Jul"~7,
                                                                             Month_Chr=="Aug"~8,
                                                                             Month_Chr=="Sep"~9,
                                                                             Month_Chr=="Oct"~10,
                                                                             Month_Chr=="Nov"~11,
                                                                             Month_Chr=="Dec"~12)); toc()

tic(); pivot_monthly_EVI<- pivot_monthly_EVI %>% group_by(cell, Year) %>% arrange(Month_Num, .by_group = TRUE); toc()

tic(); pivot_monthly_NDVI<- monthly_NDVI_df %>% pivot_longer(4:255); toc() 
tic(); pivot_monthly_NDVI<- pivot_monthly_NDVI %>% separate_wider_delim(cols = name, delim = "_", names = c("Year","Month_Chr"));toc()
tic(); pivot_monthly_NDVI<- pivot_monthly_NDVI %>% mutate(Month_Num= case_when(Month_Chr=="Jan"~1,
                                                                             Month_Chr=="Feb"~2,
                                                                             Month_Chr=="Mar"~3,
                                                                             Month_Chr=="Apr"~4,
                                                                             Month_Chr=="May"~5,
                                                                             Month_Chr=="Jun"~6,
                                                                             Month_Chr=="Jul"~7,
                                                                             Month_Chr=="Aug"~8,
                                                                             Month_Chr=="Sep"~9,
                                                                             Month_Chr=="Oct"~10,
                                                                             Month_Chr=="Nov"~11,
                                                                             Month_Chr=="Dec"~12)); toc()

tic(); pivot_monthly_NDVI<- pivot_monthly_NDVI %>% group_by(cell, Year) %>% arrange(Month_Num, .by_group = TRUE); toc()


#Applying Berdugo's criteria of a pixel having mean annual value=NA if more than
#10/23 values are missing (no mention in Berdugo whether missing values are consecutive or total)
tic(); pivot_annual_EVI <- pivot_monthly_EVI %>% group_by(cell, Year) %>% 
  summarise(Num_NA= sum(is.na(value)),
            value_mean= ifelse(Num_NA>(10/23)*12, NA, mean(value, na.rm=T))); toc()

#write_rds(pivot_annual_EVI, here("Outputs", "Indices", "Caatinga", "pivot_condition_annual_EVI.rds"))

tic(); pivot_annual_NDVI <- pivot_monthly_NDVI %>% group_by(cell, Year) %>% 
  summarise(Num_NA= sum(is.na(value)),
            value_mean= ifelse(Num_NA>(10/23)*12, NA, mean(value, na.rm=T))); toc()

#write_rds(pivot_annual_EVI, here("Outputs", "Indices", "Caatinga", "pivot_condition_annual_EVI.rds"))
#write_rds(pivot_annual_NDVI, here("Outputs", "Indices", "Caatinga", "pivot_condition_annual_NDVI.rds"))
remove(read_rasters_function, annual_EVI, annual_NDVI, monthly_EVI, monthly_NDVI)
gc()
```


#2. Classification trajectory
First code chunk is the classification function
```{r}
library(MuMIn)

annual_class_trajectory_mod <- function (dataset = NULL,interval_size = 0.5) {
  dataset<- dataset %>% mutate(time_step= 1:nrow(dataset))
  #Trajectory fitting
  null_mod<- lm(value_mean ~ 1, data = dataset) ##1. No intercept
  summary(null_mod) #significant pvalue next to intercept  means that the mean is significantly different from 0
  nmrs_null<- sqrt(sum(summary(null_mod)$residuals^2,na.rm=T)/length(dataset$value_mean))/sd(dataset$value_mean,na.rm=T)
  aic_null<- MuMIn::AICc(null_mod)

  linear_mod<- lm(value_mean ~ time_step, data = dataset) ##2.linear
  summary(linear_mod)
  nmrs_lin<- sqrt(sum(summary(linear_mod)$residuals^2,na.rm=T)/length(dataset$value_mean))/sd(dataset$value_mean,na.rm=T)
  aic_lin<- MuMIn::AICc(linear_mod)

  orth_poly_mod<- lm(value_mean ~ poly(time_step,2, raw=F), data = dataset) #raw=F means orthogonal polynomials are used
  summary(orth_poly_mod)
  nmrs_quad<- sqrt(sum(summary(orth_poly_mod)$residuals^2,na.rm=T)/length(dataset$value_mean))/sd(dataset$value_mean, na.rm=T)
  aic_quad<- MuMIn::AICc(orth_poly_mod)

  ## To get relevant values for quadratic output - Directly from Pellise et al., 2024 
  ## https://github.com/matpelissie/abrupt_shifts_ecological_timeseries_classification/blob/main/R/functions_trajclass.R
  
  # After getting Y = gamma*chi + delta*X' + epsilon with orthogonal polynomial,
  # we have to perform a variable change to obtain relevant values in the X interval 
  # for first_order_coefficient, second_order_coefficient and intercept, 
  # knowing that X'= alpha*X + beta and chi = eta*X'^2 + theta

  gammab  <-  orth_poly_mod$coefficients[3]
  delta  <-  orth_poly_mod$coefficients[2]
  epsilon  <-  orth_poly_mod$coefficients[1]


  alpha  <-  lm(orth_poly_mod$model[, 2][, 1] ~ dataset$time_step[!is.na(dataset$value_mean)])$coef[2]
  beta  <-  lm(orth_poly_mod$model[, 2][, 1] ~ dataset$time_step[!is.na(dataset$value_mean)])$coef[1]

  eta  <-  1/lm((orth_poly_mod$model[, 2][, 1])^2 ~
                orth_poly_mod$model[, 2][, 2])$coef[2]
  theta  <-  (-lm((orth_poly_mod$model[, 2][, 1])^2 ~
                  orth_poly_mod$model[, 2][, 2])$coef[1])*eta

    Y2 <- dataset$value_mean[!is.na(dataset$value_mean)]*(max(dataset$time_step[!is.na(dataset$value_mean)])-min(dataset$time_step[!is.na(dataset$value_mean)]))/(max(dataset$value_mean[!is.na(dataset$value_mean)])-min(dataset$value_mean[!is.na(dataset$value_mean)])) 

  # p2 and p3 are relevant when Y and X amplitudes are equivalent,in particular when 
  # studying scaled-to-1 indices, Y and X amplitudes may be very different, so we 
  # scaled the amplitudes to calculate p2 and p3

  polynomial_orthonormal_basis <- lm(Y2~poly(dataset$time_step[!is.na(dataset$value_mean)],2, raw=T))$coefficients

  # Quadratic model output:
  classification <-
    data.frame(first_order_coefficient = (delta+2*beta*gammab*eta)*alpha,
               first_order_pvalue =
                 summary(orth_poly_mod)$coefficients[2, 4],
               second_order_coefficient = (alpha^2)*gammab*eta,
               second_order_pvalue =
                 summary(orth_poly_mod)$coefficients[3, 4],
               strd_error=summary(orth_poly_mod)$coefficients[2, 2],
               intercept = epsilon+beta*delta+(beta^2)*gammab*eta+gammab*theta,
               x_m = (dataset$time_step[length(dataset$time_step)]-dataset$time_step[1])/2+dataset$time_step[1],
               # points of interest:
               p1 = -(delta+2*beta*gammab*eta)/(2*alpha*gammab*eta),
               p2 = (-polynomial_orthonormal_basis[2]+1)/
                 (2*polynomial_orthonormal_basis[3]),
               p3 = (-polynomial_orthonormal_basis[2]-1)/
                 (2*polynomial_orthonormal_basis[3]),
               aic = aic_quad,
               nrmse = nmrs_quad,
               loc_brk = NA, 
               trend = NA,
               mag = NA,
               rel_chg = NA,
               SDbef = NA,
               SDaft = NA)
  
  # Linear model output:
  classification[2,] <-
    data.frame(first_order_coefficient = delta*alpha,
               first_order_pvalue =
                 summary(orth_poly_mod)$coefficients[2, 4],
               second_order_coefficient = 0,
               second_order_pvalue =
                 summary(orth_poly_mod)$coefficients[3, 4],
               strd_error=summary(orth_poly_mod)$coefficients[2, 2],
               intercept = epsilon+delta*beta,
               x_m = (dataset$time_step[length(dataset$time_step)]-dataset$time_step[1])/2+dataset$time_step[1],
               p1 = NA,
               p2 = NA,
               p3 = NA,
               aic = aic_lin,
               nrmse = nmrs_lin,
               loc_brk = NA, 
               trend = NA,
               mag = NA,
               rel_chg = NA,
               SDbef = NA,
               SDaft = NA)

  # No change model output:
  classification[3,] <-
    data.frame(first_order_coefficient = 0,
               first_order_pvalue =
                 summary(orth_poly_mod)$coefficients[2, 4],
               second_order_coefficient = 0,
               second_order_pvalue =
                 summary(orth_poly_mod)$coefficients[3, 4],
               strd_error=summary(orth_poly_mod)$coefficients[2, 2],
               intercept = null_mod$coefficients,
               x_m = (dataset$time_step[length(dataset$time_step)]-dataset$time_step[1])/2+dataset$time_step[1],
               p1 = NA,
               p2 = NA,
               p3 = NA,
               aic = aic_null,
               nrmse = nmrs_null,
               loc_brk = NA, 
               trend = NA,
               mag = NA,
               rel_chg = NA,
               SDbef = NA,
               SDaft = NA)


  # Classification of each model fitted:
  for (i in 1:3){

    # Compute the derivative at xm-delta and at xm + delta with delta being
    # half of the input interval size (its 25% of the time interval which is set as 0.5)
      derivative <-
        2*(classification$x_m[i] - (dataset$time_step[length(dataset$time_step)]-dataset$time_step[1])*(interval_size/2))*  
        classification$second_order_coefficient[i] +
        classification$first_order_coefficient[i]
      derivative2 <-
        2*(classification$x_m[i] + (dataset$time_step[length(dataset$time_step)]-dataset$time_step[1])*(interval_size/2))*
        classification$second_order_coefficient[i] +
        classification$first_order_coefficient[i]


      if(sign(derivative) != sign(derivative2) | i==3){
      # non consistent direction around x_m
        classification$derivative[i]  <-  NA
        classification$intercept_derivative[i]  <-  NA

      } else {
      # consistent direction around x_m
        classification$derivative[i] <- mean(c(derivative, derivative2))
        classification$intercept_derivative[i] <-
          (classification$second_order_coefficient[i]*classification$x_m[i]^2+
             classification$first_order_coefficient[i]*classification$x_m[i]+
              classification$intercept[i]) -
          classification$x_m[i]*classification$derivative[i]
      }

    # Compute the derivative of the curvature function to get acceleration:
      classification$derivated_curvature[i] <-
        -12*(classification$second_order_coefficient[i]^2)*
        (2*classification$second_order_coefficient[i]*classification$x_m[i]+
           classification$first_order_coefficient[i])*
        (classification$second_order_coefficient[i]/
           abs(classification$second_order_coefficient[i]))/
        ((1+(2*classification$second_order_coefficient[i]*classification$x_m[i]+
            classification$first_order_coefficient[i])^2)^(2.5))

    # Keep derivated curvature even if not significant for polynomial fit:
      if(classification$second_order_pvalue[i]>0.05 & i != 1){
        classification$derivated_curvature[i] <- NA
      }

    # Classify the direction:
      classification$direction[i] <- NA
        classification$direction[i][which(
          classification$derivative[i] > 0)] <- "increase"
      classification$direction[i][which(
        classification$derivative[i] < 0)] <- "decrease"
      classification$direction[i][which(
        is.na(classification$derivative[i]))] <- "stable"
      classification$direction[i][which(
        as.numeric(classification$first_order_pvalue[i])>0.05 &
          as.numeric(classification$second_order_pvalue[i])>0.05)] <- "stable"

    # Classify the acceleration:
      classification$acceleration[i] <- NA
        classification$acceleration[i][which(
          classification$derivated_curvature[i] < 0)] <- "accelerated"
      classification$acceleration[i][which(
        classification$derivated_curvature[i] > 0)] <- "decelerated"
      classification$acceleration[i][which(
        classification$direction[i] == "stable" &
          classification$second_order_coefficient[i] < 0)] <- "concave"
      classification$acceleration[i][which(
        classification$direction[i] == "stable" &
          classification$second_order_coefficient[i] > 0)] <- "convex"
      classification$acceleration[i][which(
        is.na(classification$derivated_curvature[i]))] <- "constant"

    # Give the final classification combining direction and acceleration:
      classification$shape_class[i] <- paste(classification$direction[i],
                                             classification$acceleration[i],
                                             sep="_")
  }
  
  # Abrupt breakpoint analyses
  chng_fit<- chngpt::chngptm(formula.1=value_mean~1,
                      formula.2=~time_step,
                      family="gaussian", data=dataset,
                      type="step",
                      var.type="bootstrap", weights=NULL)

  pred_chg <- data.frame(timestep = dataset$time_step[!is.na(dataset$value_mean)],
                         bp = chng_fit$best.fit$fitted.values)

  chng_nrmse <- sqrt(sum(residuals(chng_fit)^2)/length(dataset$value_mean))/sd(dataset$value_mean)

  classification[4, 13] <- chng_fit$chngpt
  classification[4, 11] <- MuMIn::AICc(chng_fit)
  classification[4, 12] <- chng_nrmse
  classification[4, 14] <- ifelse(pred_chg$bp[1] >
                                       pred_chg$bp[length(pred_chg$bp)],
                                     "decrease", "increase")
  classification[4, 15] <-  pred_chg$bp[length(pred_chg$bp)] - pred_chg$bp[1]
  classification[4, 16] <-  (pred_chg$bp[length(pred_chg$bp)] - pred_chg$bp[1]) /
                        max(abs(pred_chg$bp[length(pred_chg$bp)]),abs(pred_chg$bp[1]))
  classification[4, 17] <-  dataset %>%
                                  dplyr::filter(time_step<=chng_fit$chngpt) %>%
                                  dplyr::pull(value_mean) %>%
                                  sd()
  classification[4, 18] <- dataset %>%
                        dplyr::filter(time_step>=chng_fit$chngpt) %>%
                        dplyr::pull(value_mean) %>%
                        sd()
  
  row.names(classification) <- c("Y_pol","Y_lin", "Y_nch", "Y_abpt")
  
  classification$cell <- unique(dataset$cell)
  #classification$x <- unique(dataset$x)
  #classification$y <- unique(dataset$y)

  return(classification)
}
```

Applying classification function for annual anisoEVI
```{r}
pivot_annual_EVI<- read_rds(here("Outputs", "Indices", "Caatinga", "pivot_condition_annual_EVI.rds"))
#there are a couple of pixels that have only 1 or 2 annual values after aggregating the monthly values
yearNAannualEVI <- pivot_annual_EVI %>% group_by(cell) %>% summarise(Year_NA=sum(is.na(value_mean)))
percent_yearNA_annualEVI<- yearNAannualEVI %>% group_by(Year_NA)%>% summarise(count=n(),
                                                                               percentage=(count/823841)*100) #823841 is number of unique pixels

#About 98% of all pixels have no missing annual values or have only 1 missing annual value
#I apply Berdugo rule again and keep pixels that have (10/23)*21 less than or equal to 9 annual values only

tic();pivot_annual_EVI2<- pivot_annual_EVI %>% group_by(cell) %>% 
  mutate(Year_NA= sum(is.na(value_mean))) %>%
  filter(Year_NA<=((10/23)*21)); toc()

#splitting into a list of dataframes
chunk2<- 21*10000
n2<- nrow(pivot_annual_EVI2)
r2<- rep(1:ceiling(n2/chunk2), each=chunk2)[1:(n2)]
tic(); split_df<- split(pivot_annual_EVI2, r2); toc()
remove(chunk2, n2, r2)

#Sys.time(); for ( i in 1:length(split_df)){
#  write_rds(split_df[[i]], here("Outputs", "Indices", "Caatinga", "split_groups_annualEVI",paste0("split_df_annualEVI_",i, ".rds")))
#}; Sys.time()

numCores<- 32
library(foreach)
library(doParallel)

my.cluster<- parallel::makeCluster(
  numCores,
  type = "FORK",
  outfile = ""
)

doParallel::registerDoParallel(cl = my.cluster)
foreach::getDoParRegistered() #should be TRUE
foreach::getDoParWorkers() #should give numCores

Sys.time(); x_annualEVI<- foreach(
  i= 1:length(split_df),
  .combine= "rbind") %dopar%
    (split_df[[i]] %>%
      group_by(cell) %>%
      nest() %>%
      mutate(classification_data = purrr::map(data, annual_class_trajectory_mod)) %>%
      select(-data) %>%
      unnest(classification_data)
       ); Sys.time()
write_rds(x_annualEVI, here("Outputs", "Trajectory_Classification", "Caatinga", "classification_annualEVI.rds"))
remove(split_df, pivot_annual_EVI, pivot_annual_EVI2, percent_yearNA_annualEVI, yearNAannualEVI, x_annualEVI)
```


Applying classification function for annual NDVI
```{r}
pivot_annual_NDVI<- read_rds(here("Outputs", "Indices", "Caatinga", "pivot_condition_annual_NDVI.rds"))
#there are a couple of pixels that have only 1 or 2 annual values after aggregating the monthly values
yearNAannualNDVI <- pivot_annual_NDVI %>% group_by(cell) %>% summarise(Year_NA=sum(is.na(value_mean)))
percent_yearNA_annualNDVI<- yearNAannualNDVI %>% group_by(Year_NA)%>% summarise(count=n(),
                                                                               percentage=(count/nrow(yearNAannualNDVI))*100) #823841 is number of unique pixels

#About 98% of all pixels have no missing annual values or have only 1 missing annual value
#I apply Berdugo rule again and keep pixels that have (10/23)*21 less than or equal to 9 annual values only

tic();pivot_annual_NDVI2<- pivot_annual_NDVI %>% group_by(cell) %>% 
  mutate(Year_NA= sum(is.na(value_mean))) %>%
  filter(Year_NA<=((10/23)*21)); toc()

#splitting into a list of dataframes
chunk2<- 21*10000
n2<- nrow(pivot_annual_NDVI2)
r2<- rep(1:ceiling(n2/chunk2), each=chunk2)[1:(n2)]
tic(); split_df<- split(pivot_annual_NDVI2, r2); toc()
remove(chunk2, n2, r2)

#Sys.time(); for ( i in 1:length(split_df)){
#  write_rds(split_df[[i]], here("Outputs", "Indices", "Caatinga", "split_groups_annualNDVI",paste0("split_df_annualNDVI_",i, ".rds")))
#}; Sys.time()

numCores<- 32
library(foreach)
library(doParallel)

my.cluster<- parallel::makeCluster(
  numCores,
  type = "FORK",
  outfile = ""
)

doParallel::registerDoParallel(cl = my.cluster)
foreach::getDoParRegistered() #should be TRUE
foreach::getDoParWorkers() #should give numCores

Sys.time(); x_annualNDVI<- foreach(
  i= 1:length(split_df),
  .combine= "rbind") %dopar%
    (split_df[[i]] %>%
      group_by(cell) %>%
      nest() %>%
      mutate(classification_data = purrr::map(data, annual_class_trajectory_mod)) %>%
      select(-data) %>%
      unnest(classification_data)
       ); Sys.time()
write_rds(x_annualNDVI, here("Outputs", "Trajectory_Classification", "Caatinga", "classification_annualNDVI.rds"))
remove(split_df, pivot_annual_NDVI, pivot_annual_NDVI2, percent_yearNA_annualNDVI, yearNAannualNDVI, x_annualNDVI)



```