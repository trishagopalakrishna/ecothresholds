---
title: "Generating the dataset"
author: "Miguel Berdugo"
date: "10/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Typology clasification

This code shows how the typology classification is processed.

### Opening the database

```{r prep,echo=TRUE,eval=TRUE}
library(dplyr)
library(ggplot2)
library(reshape2)
library(maptools)
library(sf)
library(rnaturalearth)
library(rnaturalearthdata)
library(gridExtra)
library(chngpt)
library(parallel)


df2=read.csv("Time Series new.csv",) #Time Series is NDVI data. Columns 1:19 are NDVI, column nn is number of valid data and column Point.ID is the identifier if the point.
#To obtain this dataset download data directly from MODIS using Google Earth Engine. For replication purposes ask the corresponding author to send the file directly

dff=df2[,c(22,21,1:19)]
Sites=unique(dff$Point.ID)

```

### Processing types of trajectories

Lets try to find the typology of trajectories by using AIC criteria and see whether something interesting emerges. In this case lets stick to cases where we have, at least 75% of information (i.e., at least 12 years) and we are only interested in NDVI trends

```{r selectcasesNDVI, echo=TRUE,eval=FALSE}

table(dff$nn)

```

Once identified lets do a very simple approach. We fit a linear, a quadratic and a step regression and save AICs. I also compare those and select the best fitting in a factor variable. Note that I standardize NDVI values before fitting them. I do it only with the bootstrap:


```{r typestrajectoriesBOOT, echo=TRUE,eval=FALSE}

ids=dff$Point.ID[dff$nn>=15]  #only use cases with 16 or more valid years

#Prepare parallel
noCores=detectCores()-2
cl=makeCluster(noCores)
clusterEvalQ(cl,library("chngpt"))


dfres=parLapply(cl,ids, function(x,df){
  dfi=data.frame(year=seq(from=2001, to=2019),NDVI=as.numeric(df[df$Point.ID==x,c(3:21)])) #Build the dataset for the site time series
  dfi=as.data.frame(lapply(dfi, function(y){return((y-mean(y,na.rm = TRUE))/sd(y,na.rm = TRUE))})) #Standardization of variables
  dfi$md=mahalanobis(dfi,center = apply(dfi[,1:2], MARGIN = 2, FUN=function(x) return(median(x,na.rm = TRUE))),cov = cov(dfi,use = "complete.obs")) #Calculates mahalanobis distance
  dfi=dfi[complete.cases(dfi),]
  dfi$md[dfi$md<12]=6 #Enforce md<12 to be 6; these are most likely chosen as have no significant MD (significance level of MD following a chisquared distribution is 12)

  dfresi=lapply(1:100, function(yy,dd) {
      #boot sample
      dd=dd[sample(1:nrow(dd),nrow(dd),replace = TRUE,prob = 1/dd$md),]
      #fitting models
      mdl0=glm(data=dd,formula=NDVI~1,family = "gaussian")
      mdl1=glm(data=dd,formula=NDVI~year,family = "gaussian")
      mdl2=glm(data = dd,formula = NDVI~poly(year,degree = 2),family = "gaussian")
      mdl3=chngptm(formula.1 = NDVI~1,formula.2 = ~year,data = dd,type="step",family = "gaussian")
      
      #Model selection
      aics=c(aic.notrend=mdl0$aic,aic.Lineal=mdl1$aic,aic.Quad=mdl2$aic,aic.Step=mdl3$best.fit$aic) #build AICs
      aics=aics[order(aics)] #ordered aics
      daics=aics-min(aics) #delta AICs to see their corresponding difference with the best model
      complexity=c("aic.notrend","aic.Lineal","aic.Step","aic.Quad") #model complexity from less to most complex

      daics[daics<2]=0 #delta aics lower than 2 are set to 0
      bf=complexity[min(match(names(daics[daics==0]),complexity))] #decide among daics = 0 which is the less complex
      bf=strsplit(bf,".",fixed = TRUE)[[1]][2] #extract the name of the best fit
      
      
      #The whole process of model selection is repeated without taking into account steps
      aicsLQ=c(aic.notrend=mdl0$aic,aic.Lineal=mdl1$aic,aic.Quad=mdl2$aic) #this is not taking into account steps
      aicsLQ=aicsLQ[order(aicsLQ)]
      daicsLQ=aicsLQ-min(aicsLQ)
      complexity2=c("aic.notrend","aic.Lineal","aic.Quad")

      daicsLQ[daicsLQ<2]=0
      bfLQ=complexity2[min(match(names(daicsLQ[daicsLQ==0]),complexity2))]
      bfLQ=strsplit(bfLQ,".",fixed = TRUE)[[1]][2]      

      
      coffall=c(a=NA,b=NA,stp=NA,chnpt=NA) #allocate a vector of parameters
      coffLQ=c(aLQ=NA,bLQ=NA) #and another one without steps
      
      #Set parameter values of the best model
      if(bf=="Lineal"){
        coffall[2]=mdl1$coefficients[2]
      }else if (bf=="Quad"){
        coffall[1]=mdl2$coefficients[3]
        coffall[2]=mdl2$coefficients[2]
      } else if (bf=="Step"){
        coffall[3]=mdl3$best.fit$coefficients[2]
        coffall[4]=mdl3$coefficients[3]
      }
      
      #The same without taking into account steps
      if(bfLQ=="Lineal"){
        coffLQ[2]=mdl1$coefficients[2]
      }else if (bfLQ=="Quad"){
        coffLQ[1]=mdl2$coefficients[3]
        coffLQ[2]=mdl2$coefficients[2]
      }
      
      #generate vector of results
      vv=as.data.frame(t(c(btID=yy,coffall,coffLQ)))
      vv$bestfit=bf
      vv$bestfitLQ=bfLQ
      return(vv)
  },dd=dfi)
  dfresi=as.data.frame(do.call(rbind,dfresi)) #this contains results of all bootstraps for this site
  
  bff=table(dfresi$bestfit)  #check which is the best fit and how many times it is
  bf=names(bff[which.max(bff)])
  certainty.bf=max(bff)/sum(bff) #certainty of best model selection
  coff=c(a=NA,b=NA,stp=NA,aci=NA,bci=NA,stpci=NA,sdchngpt=NA,chngpt=NA) #allocate for this site parameters results
  #calculate medians of parameters of the best fit and confidence intervals
  if(bf=="Lineal"){
    coff[2]=median(dfresi$b[dfresi$bestfit=="Lineal"])
    coff[5]=median(dfresi$b[dfresi$bestfit=="Lineal"])-quantile(dfresi$b[dfresi$bestfit=="Lineal"],0.05)
  } else if (bf=="Quad"){
    coff[1]=median(dfresi$a[dfresi$bestfit=="Quad"])
    coff[2]=median(dfresi$b[dfresi$bestfit=="Quad"])
    coff[4]=median(dfresi$a[dfresi$bestfit=="Quad"])-quantile(dfresi$a[dfresi$bestfit=="Quad"],0.05)
    coff[5]=median(dfresi$b[dfresi$bestfit=="Quad"])-quantile(dfresi$b[dfresi$bestfit=="Quad"],0.05)
  } else if (bf=="Step"){
    coff[3]=median(dfresi$stp[dfresi$bestfit=="Step"])
    coff[6]=median(dfresi$stp[dfresi$bestfit=="Step"])-quantile(dfresi$stp[dfresi$bestfit=="Step"],0.05)
    coff[7]=sd(dfresi$chnpt[dfresi$bestfit=="Step"],na.rm = TRUE)
    coff[8]=median(dfresi$chnpt[dfresi$bestfit=="Step"],na.rm = TRUE)
  }
  
  #We do the same without taking into account steps
  bffLQ=table(dfresi$bestfitLQ)
  bfLQ=names(bffLQ[which.max(bffLQ)])
  certainty.bfLQ=max(bffLQ)/sum(bffLQ)
  coffLQ=c(aLQ=NA,bLQ=NA,aciLQ=NA,bciLQ=NA)
  if(bfLQ=="Lineal"){
    coffLQ[2]=median(dfresi$bLQ[dfresi$bestfitLQ=="Lineal"])
    coffLQ[4]=median(dfresi$bLQ[dfresi$bestfitLQ=="Lineal"])-quantile(dfresi$bLQ[dfresi$bestfitLQfit=="Lineal"],0.05)
  }else if (bfLQ=="Quad"){
    coffLQ[1]=median(dfresi$aLQ[dfresi$bestfitLQ=="Quad"])
    coffLQ[2]=median(dfresi$bLQ[dfresi$bestfitLQ=="Quad"])
    coffLQ[3]=median(dfresi$aLQ[dfresi$bestfitLQ=="Quad"])-quantile(dfresi$aLQ[dfresi$bestfitLQ=="Quad"],0.05)
    coffLQ[4]=median(dfresi$bLQ[dfresi$bestfitLQ=="Quad"])-quantile(dfresi$bLQ[dfresi$bestfitLQ=="Quad"],0.05)
  }
  
  vvr=as.data.frame(t(c(Point.ID=x,certainty.bf=certainty.bf,certainty.bfLQ=certainty.bfLQ,coff,coffLQ))) #final vector of results
  vvr$bestfit=bf
  vvr$bestfitLQ=bfLQ
  
  return(vvr)
  
},df=dff)

stopCluster(cl)

dfres2=as.data.frame(do.call(rbind,dfres)) #final database of clasification

#Data are saved so that the computation is not performed again. You may find the results as an attached database at figshare
write.csv(dfres2,"MiguelResTypesMD.csv")

```

Resulting file contains the best fit clasification, both taking and without taking into account step behaviors. We did the procedure without taking into account steps because if the clasification do not fullfill some criteria (next section) they will be re-classified directly as the best model among LQ types (without taking into acount steps).

Right now the best fits are clasified as: notrend, lineal trend, quadratic trend or step trend, without attending to whether this trend is positive, negative or neutral (which will be assessed afterwards based on parameters stored)

### Further filtering step behaviors

Steps are further filtered based on the next criteria:

#### Criteria 1: The extremes

Steps whose change point occurs in the extremes (2000-2003 or 2016-2019) are discarded.

#### Criteria 2: How standard deviation on changepoints influence certainty

Steps whose change point standard deviation (as calculated during bootstrap) compromise certainty in clasification are disregarded.
To know which standard deviation in change point was compromising certainty we related both parameters. We found a breakpoint in their relationship on sdchangepoint=0.68. From that sdchangepoint on certainty stabilized as very low.
```{r adchangepont, eval=TRUE,echo=TRUE}
dfres=read.csv("MiguelResTypesMD.csv") #This file is provided in the figshare.

dfres$type=1
dfres$type[dfres$sdchngpt>0.6856748]=2
  
p2<-ggplot(data = dfres[dfres$bestfit=="Step",],aes(x=sdchngpt,y=certainty.bf))+
  theme_classic()+
  geom_point(size=0.1)+
  geom_smooth(method = "lm",aes(group=type),color="black")+
  geom_density2d()

p2
```

Please note in the figure that still the left part of the fitting is not tracking a straight line of the most dense point cloud. This occurs because still there are points with low certainty in the left side  that are draging the fit. To account for this we additionally not regarded steps that have certainty lower than 0.7.

#### Criteria 3: Only real regime shifts

Steps in which the values of NDVI before and after the change point are not statistically different should be disregarded, as those do not match proper definition of regime shift (i.e., changes in structure and/or functioning fundamentally changed respect to original state)

To assess this, we did a re-check of step behaviors using a wald test.

```{r recheck bimod, eval=FALSE,echo=TRUE}
library(diptest)
Dynamic=read.csv("Time Series new.csv")
Dynamic=Dynamic[,c(22,21,1:19)]

sites=DF$Site[DF$TrajType=="Step"]

res=lapply(sites, function(x,df,dfres){
  dfi=data.frame(year=seq(from=2001, to=2019),NDVI=as.numeric(df[df$Point.ID==x,c(3:21)]))
  dfi=as.data.frame(lapply(dfi, function(y){return((y-mean(y,na.rm = TRUE))/sd(y,na.rm = TRUE))})) #Standardization of variables
  fg=dip.test(dfi$NDVI,simulate.p.value = TRUE)
  chngpt=dfres$chngpt[dfres$Point.ID==x]
  x1=dfi$NDVI[dfi$year<chngpt]
  x2=dfi$NDVI[dfi$year>chngpt]
  fg2=ks.test(x1,x2)
  pvalKS=fg2$p.value
  return(data.frame(site=x,Dstatistic=fg$statistic,Pval=fg$p.value,pvalKS=pvalKS))
},df=Dynamic,dfres=dfres)
res=do.call(rbind,res)

hist(log(res$Pval))
write.csv(res,file="rechecksteps.csv")
```

## Building the final dataset

### Re-clasifying cases that do not meet criteria for being steps

```{r buildDF}

dfres=read.csv("C:\\Users\\velai\\Dropbox\\Drydin\\Datanew\\MiguelResTypesMD.csv") #the classified typologies file
recheck=read.csv("C:\\Users\\velai\\Dropbox\\Drydin\\Datanew\\rechecksteps.csv") #the results of the KS test comparing NDVI prior/after change point
recheck$invalid=recheck$pvalKS>0.05 #not significant differences on NDVI are marked as invalid
invalids=recheck$site[recheck$invalid=="TRUE"]


#put as type=2 those sites to be re-clasified
dfres$type=1
dfres$type[dfres$chngpt<(-1)]=2 #Meet criteria 1 (left side)
dfres$type[dfres$chngpt>(1)]=2 #Meet criteria 1 (right side)
dfres$type[dfres$sdchngpt>0.6856748]=2 #Meet Criteria 2
dfres$type[dfres$bestfit=="Step" && dfres$certainty.bf<0.7]=2 #Meet criteria 2
dfres$type[dfres$Point.ID %in% invalids]=2 #Meet criteria 3

```

### Assigning classes of shapes and trends

We assign classes of shape and trends as said in the Methods section

```{r assigning classes}
dfres$subtypes=NA
dfres$subtypes[dfres$bestfit=="Step" & dfres$stp>0]="Positive Step"
dfres$subtypes[dfres$bestfit=="Step" & dfres$stp<0]="Negative Step"
dfres$subtypes[dfres$bestfit=="Lineal" & dfres$b<0]="Negative Linear"
dfres$subtypes[dfres$bestfit=="Lineal" & dfres$b>0]="Positive Linear"
dfres$subtypes[dfres$bestfit=="Quad" & dfres$a>0 & (dfres$b- dfres$bci)>0]="Positive Quad"
dfres$subtypes[dfres$bestfit=="Quad" & dfres$a<0 & (dfres$b+dfres$bci)<0]="Negative Quad"
dfres$subtypes[dfres$bestfit=="Quad" & dfres$a<0 & (dfres$b-dfres$bci)>0]="Positive Quad"
dfres$subtypes[dfres$bestfit=="Quad" & dfres$a>0 & (dfres$b+dfres$bci)<0]="Negative Quad"
dfres$subtypes[dfres$bestfit=="Quad" & dfres$a<0 & (dfres$b-dfres$bci)<0 & (dfres$b+dfres$bci)>0]="Shifting"
dfres$subtypes[dfres$bestfit=="Quad" & dfres$a>0 & (dfres$b-dfres$bci)<0 & (dfres$b+dfres$bci)>0]="Shifting"
dfres$subtypes[dfres$bestfit=="notrend"]="notrend"

#do the same for cases not taking into account steps
dfres$subtypesLQ=NA
dfres$subtypesLQ[dfres$bestfitLQ=="Lineal" & dfres$bLQ<0]="Negative Linear"
dfres$subtypesLQ[dfres$bestfitLQ=="Lineal" & dfres$bLQ>0]="Positive Linear"
dfres$subtypesLQ[dfres$bestfitLQ=="Quad" & dfres$aLQ>0 & (dfres$bLQ- dfres$bciLQ)>0]="Positive Quad"
dfres$subtypesLQ[dfres$bestfitLQ=="Quad" & dfres$aLQ<0 & (dfres$bLQ+dfres$bciLQ)<0]="Negative Quad"
dfres$subtypesLQ[dfres$bestfitLQ=="Quad" & dfres$aLQ<0 & (dfres$bLQ-dfres$bciLQ)>0]="Positive Quad"
dfres$subtypesLQ[dfres$bestfitLQ=="Quad" & dfres$aLQ>0 & (dfres$bLQ+dfres$bciLQ)<0]="Negative Quad"
dfres$subtypesLQ[dfres$bestfitLQ=="Quad" & dfres$aLQ<0 & (dfres$bLQ-dfres$bciLQ)<0 & (dfres$bLQ+dfres$bciLQ)>0]="Shifting"
dfres$subtypesLQ[dfres$bestfitLQ=="Quad" & dfres$aLQ>0 & (dfres$bLQ-dfres$bciLQ)<0 & (dfres$bLQ+dfres$bciLQ)>0]="Shifting"
dfres$subtypesLQ[dfres$bestfitLQ=="notrend"]="notrend"

#finally assign different bestfit and subtypes to step cases that need re-clasification.
dfres$bestfit[dfres$type==2]=dfres$bestfitLQ[dfres$type==2]
dfres$subtypes[dfres$type==2]=dfres$subtypesLQ[dfres$type==2]

```

### Merge other databases

We finally merge data of climatic variables and do final filters to discard aridity >0.95. You can find in the manuscript where to obtain the data. Provided FinalDB3.csv file already has every data merged.

Resulting data frame is available as "FinalDB" as csv file at figshare. Some summary characteristics:
Total of cases: 41830


### Other figures associated to clasification procedure

These analysis are made to proof discontinuities in the centainty in classification of abrupt changes.
This code is computationally expensive and will take a while to run (approx 1hour in Pentium i7 with 64GB RAM)

```{r R2disc}

dfres=read.csv("MiguelResTypesMD.csv") #the classified typologies file

Dynamic=read.csv("Time Series new.csv")
Dynamic=Dynamic[,c(22,21,1:19)]

#In the subset of steps with sdchngpt<=0.68; i.e., those whose uncertainty in clasification is not driven by noisy changepoint
sites=dfres$Point.ID[dfres$bestfit=="Step" & dfres$sdchngpt<=0.6856748]

dd=dfres[dfres$Point.ID %in% sites,]

hist(dd$certainty.bf)

library(chngpt)
#Calculate R2 of the fittings
res=lapply(sites, function(x,df,dfres){
  dfi=data.frame(year=seq(from=2001, to=2019),NDVI=as.numeric(df[df$Point.ID==x,c(3:21)]))
  dfi=as.data.frame(lapply(dfi, function(y){return((y-mean(y,na.rm = TRUE))/sd(y,na.rm = TRUE))})) #Standardization of variables
  mdl3=chngptm(formula.1 = NDVI~1,formula.2 = ~year,data = dfi,type="step",family = "gaussian")
  Rsq=1-mdl3$best.fit$deviance/mdl3$best.fit$null.deviance
  conf=dfres$certainty.bf[dfres$Point.ID==x]
  
  return(data.frame(site=x,Rsq=Rsq,conf=conf))
},df=Dynamic,dfres=dd)
res=do.call(rbind,res)

library(ggplot2)

#Fit a segmented model between confidence of classification and R2 to see if there is any discontinuity
mdl=chngptm(formula.1 = Rsq~1,formula.2 = ~conf,data = res,type="segmented",family = "gaussian",save.boot = TRUE)
plot(mdl)
mdl$vcov$boot.samples
mean(mdl$vcov$boot.samples[,4])

#A discontinuity is spotted at 0.72. To plot it (Fig.S4)
res$type=1
res$type[res$conf<0.72]=2
p2<-ggplot(data = res,aes(x = conf,y=Rsq,group=type))+
  theme_classic()+
  geom_point(size=0.01)+
  ylab("R squared")+
  xlab("Confidence in Step classification")+
  geom_smooth(method="rlm")
p2

p<-ggplot(data = res,aes(x = conf))+
  theme_classic()+
  geom_histogram()+
  xlab("Confidence in Step classification")+
  geom_vline(xintercept = 0.74)
p

library(gridExtra)
fig=arrangeGrob(p,p2)
```

