---
title: "Summary of methods. Statistical analyses 1. Summaries"
author: "Miguel Berdugo"
date: "10/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Opening the database

```{r opening, echo=TRUE, eval=TRUE}
library(reshape2)
library(ggplot2)
library(dplyr)

DF=read.csv("FinalDB3.csv")
dfLC=read.csv("Landscape change.csv")
dfLC$init=ifelse(dfLC$MFLCinit %in% c(1:5),"Forest",
                 ifelse(dfLC$MFLCinit %in% c(6,7),"Shrubland",
                        ifelse(dfLC$MFLCinit==10,"Grassland",
                               ifelse(dfLC$MFLCinit %in% c(8,9),"Savannah",
                                      ifelse(dfLC$MFLCinit==16,"Desert","others")))))
dfLC$fin=ifelse(dfLC$MFLCfin %in% c(1:5),"Forest",
                ifelse(dfLC$MFLCfin %in% c(6,7),"Shrubland",
                       ifelse(dfLC$MFLCfin==10,"Grassland",
                              ifelse(dfLC$MFLCfin %in% c(8,9),"Savannah",
                                     ifelse(dfLC$MFLCfin==16,"Desert","others")))))


dfLC$change.prod=paste(as.character(dfLC$init),as.character(dfLC$fin),sep = "->")
dfLC$change.prod[dfLC$init=="others"]="others"
dfLC$change.prod[dfLC$fin=="others"]="others"
dfLC$change.prod[dfLC$fin=="Desert"]="Desertification"
dfLC$change.prod[dfLC$init=="Desert"]="de-Desertification"
dfLC$change.prod[dfLC$init==dfLC$fin]="unchanged"

dfLC$change.prod[dfLC$change.prod %in% c("Savannah->Shrubland",
                                         "Savannah->Grassland")]="Savannah->Open"
dfLC$change.prod[dfLC$change.prod %in% c("Forest->Shrubland",
                                         "Forest->Grassland")]="Forest->Open"
dfLC$change.prod[dfLC$change.prod %in% c("Shrubland->Savannah",
                                         "Grassland->Savannah")]="Open->Savannah"
dfLC$change.prod[dfLC$change.prod %in% c("Shrubland->Forest",
                                         "Grassland->Forest")]="Open->Forest"

DF$change.LC=dfLC$change.prod[match(DF$Site,dfLC$Point.ID)]

vars=c("ndvi",
       "trendPpt",
       "trendPDSI",
       "MTWQ",
       "Aridity",
       "Elevation",
       "TSE",
       "RASE",
       "CVppt",
       "MDR",
       "RCQ.perc",
       "SGsand",
       "LORC",
       "yearsdrought",
       "MTDQ",
       "change.LC",
       "WCS_Human_Footprint_2009",
       "GHS_Population_Density",
       "FanEtAl_Depth_to_Water_Table_AnnualMean",
       "SG_Depth_to_bedrock",
       "WorldClim2_SolarRadiation_AnnualMean",
       "GLW3_RuminantsDistribution",
       "lat",
       "long")
dd=DF[DF$trendtype!="Neutral",c("TrajType","trendtype",vars)] #vars are not correlated
dd$trendT=0
dd$trendT[dd$trendtype=="Positive"]=1
dd$y="Continuous"
dd$y[dd$TrajType=="Step"]="Abrupt"
dd$y=as.factor(dd$y)
dd=dd[,-c(1,2)] #delete traj and trendtype

dd=dd[complete.cases(dd),]
dd$GLW3_RuminantsDistribution=log10(dd$GLW3_RuminantsDistribution+1)
library(fastDummies)
ddi=dummy_cols(dd, select_columns = 'change.LC',remove_selected_columns = T)
ddi=ddi[,-c(which(names(ddi) %in% c("change.LC_others","change.LC_unchanged")))]


```


## Random forest of dynamical typologies

Next code will compute a xgboost model training on data of dynamical types. In short, we used two models, one for positive trends and other for negative ones. Both try to fit the probability of being step using step typologies as 1-0 (binomial regression). XGboost models first need to be parametrized. Then, we extract the SHAP values from them to study the drivers of these typologies.

### Training models. Hyperparameters

We trained the models using xgboost. We used as boosters a regression tree (gbtree), as objective binary:logistic, and as evaluation metric error rate. Then we used cross validation procedure to find the best number of trees to be trained without overfitting and a bayesian optimizer for parametrization of 6 hyperparameters. Those are:

eta: learning rate. Varying 0.1 to 0.3, controls the speed of learning of the algortihm. Usually slow rates result in more conservative models less likely to overfit.
gamma: varying 0 to 100, controls the minimum gain used to deciding a split.
max-depth: controls the number of split of the model (depth) being critical to avoid overfitting (too much depth results in overfitted models)
min_child_weight: is the number of instances weight needed in a child. If the tree partition step results in leaf node with the sum of instances weight less than this number it gives up partitioning.
subsample: is the number of samples (% from dataset) resampled in the bootstrapping procedure when initializing a new tree.
nfold: number of folds used for cross validation.
colsample_bytree: the number of predictors (%from original) resampled for a new tree.

Because the process may take a while to run, here I provide the code for doing it only for negatives. Positives will require simply to change one line. Saved results with the computed optimum parameters are provided as RData files

```{r hyperparameters, eval=FALSE,echo=TRUE}
library(dplyr)
library(xgboost)
library(reshape2)
library(ParBayesianOptimization)

##Model for negatives
df=ddi[ddi$trendT==0,-which(names(ddi)=="trendT")]  #to train positives, please do 
#df=ddi[ddi$trendT==1,-which(names(ddi)=="trendT")] #then follow the code exactly the same

coords=df[,c("lat","long")]
df=df[,-c(which(names(df) %in% c("lat","long")))] #get rid of lat and long


library(xgboost)
labels=df$y
preds=data.matrix(df[,-which(names(df)=="y")])

set.seed(200)
trainID=sample(1:nrow(df),size = floor(nrow(df)*0.7))
validid=(1:nrow(df))
validid=validid[!(validid %in% trainID)]

dtrain <- xgb.DMatrix(data = preds[trainID,], label= labels[trainID])
dtest <- xgb.DMatrix(data = preds[validid,], label= labels[validid])
dall<-xgb.DMatrix(data = preds, label= labels)

#Training super model. First, the nrounds
params_booster <- list(booster = 'gbtree', 
                       eta = 0.2, 
                       gamma = 0, 
                       max.depth = 6, 
                       subsample = 1, 
                       colsample_bytree = 1, 
                       objective = "binary:logistic",
                       eval_metric="error",
                       colsample_bytree=1)

bst.cv <- xgb.cv(data = preds[trainID,],
                 label = as.numeric(labels[trainID]=="Abrupt"), 
                 params = params_booster,
                 nrounds = 200, 
                 nfold = 5,
                 print_every_n = 20,
                 verbose = 2)
bst.cv$evaluation_log$test_error_mean
nrounds_best=which.min(bst.cv$evaluation_log$test_error_mean)

#Best hyperparameters test
set.seed(2021)

scoring_function <- function(
  eta, gamma, max_depth, min_child_weight, subsample, nfold,colsample_bytree) {
  
  #dtrain <- xgb.DMatrix(data = dtrain)
  
  pars <- list(
    eta = eta,
    gamma = gamma,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    subsample = subsample,
    colsample_bytree=colsample_bytree,
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric="error",
    verbosity = 0
  )
  
  xgbcv <- xgb.cv(
    params = pars,
    data = preds[trainID,],
    label = as.numeric(labels[trainID]=="Abrupt"),
    nfold = nfold,
    nrounds = 200,
    prediction = TRUE,
    showsd = TRUE,
    early_stopping_rounds = nrounds_best,
    maximize = TRUE
  )
  
  # required by the package, the output must be a list
  # with at least one element of "Score", the measure to optimize
  # Score must start with capital S
  # For this case, we also report the best num of iteration
  return(
    list(
      Score = -min(xgbcv$evaluation_log$test_error_mean),
      nrounds = which.min(xgbcv$evaluation_log$test_error_mean)
    )
  )
}

bounds <- list(
  eta = c(0.1, 0.3),
  gamma =c(0, 100),
  max_depth = c(3L, 10L), # L means integers
  min_child_weight = c(1, 25),
  subsample = c(0.5, 1),
  nfold = c(3L, 10L),
  colsample_bytree=c(0.5,1)
)

opt_obj <- bayesOpt(
  FUN = scoring_function,
  bounds = bounds,
  initPoints = 8,
  iters.n = 20,
)
best_param=getBestPars(optObj = opt_obj)
best_param$nround=nrounds_best

#####NOW the model parameters are set. Find them in the files Positive Hyperparameters.RData and Negative Hyperparameters.RData

```

The rest of the code should run efficiently and fast. Resulting hyperparameters are in the attached files Positive Hyperparameters.RData and Negative Hyperparameters.RData. Each contains a list with the values of each optimized parameter plus the resulting optimization summary for the curious reader (called opt_obj in the previous chunk of code).

### Training and obtaining the models

Once models are defined, to calculate accuracy, importance and shap values we used built-in functions. We now get the code for Negative steps:

```{r negativeModel}

df=ddi[ddi$trendT==0,-which(names(ddi)=="trendT")]
coords.NEG=df[,c("lat","long")]
df=df[,-c(which(names(df) %in% c("lat","long")))] #get rid of lat and long


library(xgboost)
labels=df$y
labels.NEG=labels #for later

preds=data.matrix(df[,-which(names(df)=="y")])

set.seed(200)
trainID=sample(1:nrow(df),size = floor(nrow(df)*0.7))
validid=(1:nrow(df))
validid=validid[!(validid %in% trainID)]

dtrain <- xgb.DMatrix(data = preds[trainID,], label= labels[trainID])
dtest <- xgb.DMatrix(data = preds[validid,], label= labels[validid])
dall<-xgb.DMatrix(data = preds, label= labels)

load("Negative Hyperparameters.RData")
best_param=param.Negatives

set.seed(300)
model_tuned <- xgboost(data = preds[trainID,],
                       label = as.numeric(labels[trainID]=="Abrupt"),
                       booster = "gbtree",
                       objective="binary:logistic",
                       eval_metric="error",
                       max.depth = best_param$max_depth, 
                       nround = best_param$nround, 
                       eta=best_param$eta,
                       gamma=best_param$gamma,
                       min_child_weight=best_param$min_child_weight,
                       subsample=best_param$subsample,
                       nfold=best_param$nfold,
                       colsample_bytree=best_param$colsample_bytree)



pred <- predict(model_tuned, dtest)
pred.allNEG=pred <- predict(model_tuned, dall) #this is for later


importance_matrix.NEG <- xgb.importance(names(df[,-22]), model = model_tuned)
pred <- predict(model_tuned, dtest)
pred <- ifelse(pred>0.5,1,0)
library(caret)
confusionMatrix (as.factor(pred), as.factor(as.numeric(labels[validid]=="Abrupt")))

#The SHAP

library(fastshap)
pfun <- function(object, newdata) {
  predict(object, data = newdata)$predictions
}
predsi=preds
EXPVAL.NEG=explain(model_tuned,X=predsi,pred_wrapper = pfun,exact = TRUE)
predsi.NEG=as.data.frame(predsi)


```


And for positive ones:

```{r positiveModel}
df=ddi[ddi$trendT==1,-which(names(ddi)=="trendT")]
coords.POS=df[,c("lat","long")]
df=df[,-c(which(names(df) %in% c("lat","long")))] #get rid of lat and long


library(xgboost)
labels=df$y
labels.POS=labels #for later
preds=data.matrix(df[,-which(names(df)=="y")])

set.seed(200)
trainID=sample(1:nrow(df),size = floor(nrow(df)*0.7))
validid=(1:nrow(df))
validid=validid[!(validid %in% trainID)]

dtrain <- xgb.DMatrix(data = preds[trainID,], label= labels[trainID])
dtest <- xgb.DMatrix(data = preds[validid,], label= labels[validid])
dall<-xgb.DMatrix(data = preds, label= labels)

load("Positive Hyperparameters.RData")
best_param=param.Positives

set.seed(200)
model_tuned <- xgboost(data = preds[trainID,],
                       label = as.numeric(labels[trainID]=="Abrupt"),
                       booster = "gbtree",
                       objective="binary:logistic",
                       eval_metric="error",
                       max.depth = best_param$max_depth, 
                       nround = best_param$nround, 
                       eta=best_param$eta,
                       gamma=best_param$gamma,
                       min_child_weight=best_param$min_child_weight,
                       subsample=best_param$subsample,
                       nfold=best_param$nfold,
                       colsample_bytree=best_param$colsample_bytree)



pred <- predict(model_tuned, dtest)

pred.allPOS=pred <- predict(model_tuned, dall) #this is for later


importance_matrix.POS <- xgb.importance(names(df[,-22]), model = model_tuned)
pred <- predict(model_tuned, dtest)
pred <- ifelse(pred>0.5,1,0)
library(caret)
confusionMatrix (as.factor(pred), as.factor(as.numeric(labels[validid]=="Abrupt")))


#The SHAP

library(fastshap)
pfun <- function(object, newdata) {
  predict(object, data = newdata)$predictions
}
predsi=preds
EXPVAL.POS=explain(model_tuned,X=predsi,pred_wrapper = pfun,exact = TRUE)
predsi.POS=as.data.frame(predsi)
```

SHAP values are stored, together with importance of variables. In order to obtain the plots of the paper:

Figure 2

```{r methodsSV}
##Final figures and results
##
library(gridExtra)
library(forcats)

templete=read.csv("match.csv")
importance_matrix.NEG$Feature=templete$new.var[match(importance_matrix.NEG$Feature,templete$old.var)]
importance_matrix.NEG$Importance=importance_matrix.NEG$Gain
p1<-importance_matrix.NEG %>%
  mutate(name = fct_reorder(Feature, Importance)) %>%
  ggplot( aes(x=name, y=Importance)) +
  ggtitle("Negative \nabrupt changes")+
  geom_bar(stat="identity", fill="darkred", width=.4) +
  coord_flip() +
  xlab("") +
  theme_bw()+
  theme(title = element_text(size=8),
        axis.text = element_text(size=6))



importance_matrix.POS$Feature=templete$new.var[match(importance_matrix.POS$Feature,templete$old.var)]
importance_matrix.POS$Importance=importance_matrix.POS$Gain
p2<-importance_matrix.POS %>%
  mutate(name = fct_reorder(Feature, Importance)) %>%
  ggplot( aes(x=name, y=Importance)) +
  ggtitle("Positive \nabrupt changes")+
  geom_bar(stat="identity", fill="darkblue", width=.4) +
  coord_flip() +
  xlab("") +
  theme_bw()+
  theme(title = element_text(size=8),
        axis.text = element_text(size=6))


library(gridExtra)
PP1=grid.arrange(p1, p2, nrow = 1)
#ggsave(filename = "Figure2.pdf",plot = PP1,device = "pdf",width = 14,height =7 ,units = "cm")
```

Figure 3

```{r methodsSV}
shap_long_NEG=list()
shap_long_POS=list()
for(i in 1:ncol(EXPVAL.NEG)){
  var=names(predsi.NEG)[i]
  var=templete$new.var[which(templete$old.var==var)]
  shap_long_POS[[i]]=data.frame(variable=var,value=EXPVAL.POS[[i]],rfvalue=predsi.POS[,i])
  shap_long_NEG[[i]]=data.frame(variable=var,value=EXPVAL.NEG[[i]],rfvalue=predsi.NEG[,i])
}
shap_long_POS=do.call(rbind,shap_long_POS)
shap_long_NEG=do.call(rbind,shap_long_NEG)


shap_long_NEG$Importance=importance_matrix.NEG$Importance[match(shap_long_NEG$variable,importance_matrix.NEG$Feature)]
shap_long_NEG=shap_long_NEG %>%
  mutate(name=fct_reorder(variable, Importance))
shap_long_POS$Importance=importance_matrix.POS$Importance[match(shap_long_POS$variable,importance_matrix.POS$Feature)]
shap_long_POS=shap_long_POS %>%
  mutate(name=fct_reorder(variable, Importance))

##NOW do the plot
vars=c("Precipitation variability","Rainfall seasonality","Temperature seasonality")
dfi=shap_long_NEG[shap_long_NEG$variable %in% vars,]
dfi$Type="Negative abrupt"
dfi2=shap_long_POS[shap_long_POS$variable %in% vars,]
dfi2$Type="Positive abrupt"
dfi=rbind(dfi,dfi2)

p1<-ggplot(dfi,aes(x=rfvalue,y=value,color=Type))+
  theme_classic()+
  geom_point(size=0.2,alpha=0.5)+
  scale_color_manual(values = c("darkred","darkblue"))+
  geom_smooth(color="red",data=filter(dfi,Type=="Negative abrupt"))+
  geom_smooth(color="royalblue",data=filter(dfi,Type=="Positive abrupt"))+
  xlab("\nFeature value")+
  ylab("SHAP value \n[impact on model output]\n")+
  facet_wrap(~variable,scales = "free",nrow = 1)+
  theme(strip.background = element_blank(),strip.text.x = element_blank())

vars=c("Human footprint","Aridity","Soil organic carbon","Ruminants density")
dfi=shap_long_NEG[shap_long_NEG$variable %in% vars,]
dfi$Type="Negative abrupt"
dfi2=shap_long_POS[shap_long_POS$variable %in% vars,]
dfi2$Type="Positive abrupt"
dfi=rbind(dfi,dfi2)

p2<-ggplot(dfi,aes(x=rfvalue,y=value,color=Type))+
  theme_classic()+
  geom_point(size=0.2,alpha=0.5)+
  scale_color_manual(values = c("darkred","darkblue"))+
  geom_smooth(color="red",data=filter(dfi,Type=="Negative abrupt"))+
  geom_smooth(color="royalblue",data=filter(dfi,Type=="Positive abrupt"))+
  xlab("\nFeature value")+
  ylab("SHAP value \n[impact on model output]\n")+
  facet_wrap(~variable,scales = "free",nrow = 1)+
  theme(legend.position = "none",strip.background = element_blank(),strip.text.x = element_blank())

PP2=grid.arrange(p1, p2, nrow = 2)
```

Figure S

```{r methodsSV}
dfi=shap_long_NEG
dfi$Type="Negative abrupt"
dfi2=shap_long_POS
dfi2$Type="Positive abrupt"
dfi=rbind(dfi,dfi2)

vars=c("","Precipitation variability","Rainfall seasonality","Temperature seasonality","Human footprint","Aridity","Soil organic carbon","Ruminants density")
ii=which(dfi$variable %in% vars)
dfi=dfi[-ii,]
dfi$rfvalue[dfi$variable=="Population density"]=log10(dfi$rfvalue[dfi$variable=="Population density"])

p<-ggplot(dfi,aes(x=rfvalue,y=value,color=Type))+
  theme_classic()+
  geom_point(size=0.2,alpha=0.5)+
  scale_color_manual(values = c("darkred","darkblue"))+
  geom_smooth(color="red",data=filter(dfi,Type=="Negative abrupt"))+
  geom_smooth(color="royalblue",data=filter(dfi,Type=="Positive abrupt"))+
  xlab("\nFeature value")+
  ylab("SHAP value \n[impact on model output]\n")+
  facet_wrap(~variable,scales = "free")
p
```


### Spatial autocorrelation

The existence of spatial autocorrelation may influence our results. In order to assess the importance of autocorrelation and its effect on our results we first did a semivariogram of the residuals of our models to spot the characteristic scale of each of their spatial autocorrelation signals. Then we re-do our models weighting the points by the number of other points closer tha that caracteristic scale.

First obtain semivariograms:

```{r semivariograms}
library(spdep)
library(gstat)
library(sp)
#from later we have
#pred.allNEG, pred.allPOS, coords.NEG,coords.POS, labels.POS and labels.NEG

#for negatives
lp = pred.allNEG
mu = exp(lp)/(1+exp(lp))
Y=as.numeric(labels.NEG=="Abrupt")
DevRes=sqrt(-2*log(1-mu))*sign(Y-mu)


dff=data.frame(residuals=DevRes,lat=coords.NEG$lat,long=coords.NEG$long)
coordinates(dff)=~long+lat
vv.NEG=variogram(residuals~long+lat,data = dff)

#for positives
lp = pred.allPOS
mu = exp(lp)/(1+exp(lp))
Y=as.numeric(labels.POS=="Abrupt")
DevRes=sqrt(-2*log(1-mu))*sign(Y-mu)


dff=data.frame(residuals=DevRes,lat=coords.POS$lat,long=coords.POS$long)
coordinates(dff)=~long+lat
vv.POS=variogram(residuals~long+lat,data = dff)
vv.POS$type="positive abrupt"
vv.NEG$type="Negative abrupt"

vv=rbind(vv.POS,vv.NEG)
p<-ggplot(data = vv,aes(x=dist,y=gamma))+
  theme_classic()+
  geom_point()+
  geom_smooth()+
  xlab("Distance (km)")+
  ylab("Semivariance")+
  facet_wrap(~type)
p

```

characteristic scales of negatives are 20km and positives around 50km




```{r re-do negatives}
df=ddi[ddi$trendT==0,-which(names(ddi)=="trendT")]
coords.NEG=df[,c("lat","long")]
df=df[,-c(which(names(df) %in% c("lat","long")))] #get rid of lat and long

weights=apply(as.matrix(coords.NEG), 1, function(x,dd) {
  dist=spDistsN1(dd, x, longlat = T)
  return(length(which(dist<20)))
},dd=as.matrix(coords.NEG))
weights=1/weights

library(xgboost)
labels=df$y

preds=data.matrix(df[,-which(names(df)=="y")])

set.seed(200)
trainID=sample(1:nrow(df),size = floor(nrow(df)*0.7))
validid=(1:nrow(df))
validid=validid[!(validid %in% trainID)]

dtrain <- xgb.DMatrix(data = preds[trainID,], label= labels[trainID])
dtest <- xgb.DMatrix(data = preds[validid,], label= labels[validid])
dall<-xgb.DMatrix(data = preds, label= labels)

load("Negative Hyperparameters.RData")
best_param=param.Negatives

set.seed(300)
model_tuned <- xgboost(data = preds[trainID,],
                       label = as.numeric(labels[trainID]=="Abrupt"),
                       booster = "gbtree",
                       objective="binary:logistic",
                       eval_metric="error",
                       max.depth = best_param$max_depth, 
                       nround = best_param$nround, 
                       eta=best_param$eta,
                       gamma=best_param$gamma,
                       min_child_weight=best_param$min_child_weight,
                       subsample=best_param$subsample,
                       nfold=best_param$nfold,
                       colsample_bytree=best_param$colsample_bytree,
                       weight = weights[trainID])



pred <- predict(model_tuned, dtest)


importance_matrix.NEG <- xgb.importance(names(df[,-22]), model = model_tuned)
pred <- predict(model_tuned, dtest)
pred <- ifelse(pred>0.5,1,0)
library(caret)
confusionMatrix (as.factor(pred), as.factor(as.numeric(labels[validid]=="Abrupt")))

#The SHAP

library(fastshap)
pfun <- function(object, newdata) {
  predict(object, data = newdata)$predictions
}
predsi=preds
EXPVAL.NEG=explain(model_tuned,X=predsi,pred_wrapper = pfun,exact = TRUE)
predsi.NEG=as.data.frame(predsi)

```

Now with positive model

```{r re-doPositives}
df=ddi[ddi$trendT==1,-which(names(ddi)=="trendT")]
coords.POS=df[,c("lat","long")]
df=df[,-c(which(names(df) %in% c("lat","long")))] #get rid of lat and long

weights=apply(as.matrix(coords.POS), 1, function(x,dd) {
  dist=spDistsN1(dd, x, longlat = T)
  return(length(which(dist<50)))
},dd=as.matrix(coords.POS))
weights=1/weights

library(xgboost)
labels=df$y
preds=data.matrix(df[,-which(names(df)=="y")])

set.seed(200)
trainID=sample(1:nrow(df),size = floor(nrow(df)*0.7))
validid=(1:nrow(df))
validid=validid[!(validid %in% trainID)]

dtrain <- xgb.DMatrix(data = preds[trainID,], label= labels[trainID])
dtest <- xgb.DMatrix(data = preds[validid,], label= labels[validid])
dall<-xgb.DMatrix(data = preds, label= labels)

load("Positive Hyperparameters.RData")
best_param=param.Positives

set.seed(200)
model_tuned <- xgboost(data = preds[trainID,],
                       label = as.numeric(labels[trainID]=="Abrupt"),
                       booster = "gbtree",
                       objective="binary:logistic",
                       eval_metric="error",
                       max.depth = best_param$max_depth, 
                       nround = best_param$nround, 
                       eta=best_param$eta,
                       gamma=best_param$gamma,
                       min_child_weight=best_param$min_child_weight,
                       subsample=best_param$subsample,
                       nfold=best_param$nfold,
                       colsample_bytree=best_param$colsample_bytree,
                       weight = weights[trainID])




importance_matrix.POS <- xgb.importance(names(df[,-22]), model = model_tuned)
pred <- predict(model_tuned, dtest)
pred <- ifelse(pred>0.5,1,0)
confusionMatrix (as.factor(pred), as.factor(as.numeric(labels[validid]=="Abrupt")))


#The SHAP

library(fastshap)
pfun <- function(object, newdata) {
  predict(object, data = newdata)$predictions
}
predsi=preds
EXPVAL.POS=explain(model_tuned,X=predsi,pred_wrapper = pfun,exact = TRUE)
predsi.POS=as.data.frame(predsi)
```

To check if this has affected importantly our results we plot the main figures exactly the same way we did it before

```{r rePlottingFig2}
importance_matrix.NEG$Feature=templete$new.var[match(importance_matrix.NEG$Feature,templete$old.var)]
p1<-importance_matrix.NEG %>%
  mutate(name = fct_reorder(Feature, Importance)) %>%
  ggplot( aes(x=name, y=Importance)) +
  ggtitle("Negative \nabrupt changes")+
  geom_bar(stat="identity", fill="darkred", width=.4) +
  coord_flip() +
  xlab("") +
  theme_bw()+
  theme(title = element_text(size=10),
        axis.text = element_text(size=8))



importance_matrix.POS$Feature=templete$new.var[match(importance_matrix.POS$Feature,templete$old.var)]
p2<-importance_matrix.POS %>%
  mutate(name = fct_reorder(Feature, Importance)) %>%
  ggplot( aes(x=name, y=Importance)) +
  ggtitle("Positive \nabrupt changes")+
  geom_bar(stat="identity", fill="darkblue", width=.4) +
  coord_flip() +
  xlab("") +
  theme_bw()+
  theme(title = element_text(size=10),
        axis.text = element_text(size=8))


library(gridExtra)
PP1=grid.arrange(p1, p2, nrow = 1)

```

```{r rePlottingFig3}
shap_long_NEG=list()
shap_long_POS=list()
for(i in 1:ncol(EXPVAL.NEG)){
  var=names(predsi.NEG)[i]
  var=templete$new.var[which(templete$old.var==var)]
  shap_long_POS[[i]]=data.frame(variable=var,value=EXPVAL.POS[[i]],rfvalue=predsi.POS[,i])
  shap_long_NEG[[i]]=data.frame(variable=var,value=EXPVAL.NEG[[i]],rfvalue=predsi.NEG[,i])
}
shap_long_POS=do.call(rbind,shap_long_POS)
shap_long_NEG=do.call(rbind,shap_long_NEG)


shap_long_NEG$Importance=importance_matrix.NEG$Importance[match(shap_long_NEG$variable,importance_matrix.NEG$Feature)]
shap_long_NEG=shap_long_NEG %>%
  mutate(name=fct_reorder(variable, Importance))
shap_long_POS$Importance=importance_matrix.POS$Importance[match(shap_long_POS$variable,importance_matrix.POS$Feature)]
shap_long_POS=shap_long_POS %>%
  mutate(name=fct_reorder(variable, Importance))

##NOW do the plot
vars=c("Precipitation variability","Rainfall seasonality","Temperature seasonality")
dfi=shap_long_NEG[shap_long_NEG$variable %in% vars,]
dfi$Type="Negative abrupt"
dfi2=shap_long_POS[shap_long_POS$variable %in% vars,]
dfi2$Type="Positive abrupt"
dfi=rbind(dfi,dfi2)

p1<-ggplot(dfi,aes(x=rfvalue,y=value,color=Type))+
  theme_classic()+
  geom_point(size=0.2,alpha=0.5)+
  scale_color_manual(values = c("darkred","darkblue"))+
  geom_smooth(color="red",data=filter(dfi,Type=="Negative abrupt"))+
  geom_smooth(color="royalblue",data=filter(dfi,Type=="Positive abrupt"))+
  xlab("\nFeature value")+
  ylab("SHAP value \n[impact on model output]\n")+
  facet_wrap(~variable,scales = "free",nrow = 1)+
  theme(strip.background = element_blank(),strip.text.x = element_blank())

vars=c("Human footprint","Aridity","Soil organic carbon","Ruminants density")
dfi=shap_long_NEG[shap_long_NEG$variable %in% vars,]
dfi$Type="Negative abrupt"
dfi2=shap_long_POS[shap_long_POS$variable %in% vars,]
dfi2$Type="Positive abrupt"
dfi=rbind(dfi,dfi2)

p2<-ggplot(dfi,aes(x=rfvalue,y=value,color=Type))+
  theme_classic()+
  geom_point(size=0.2,alpha=0.5)+
  scale_color_manual(values = c("darkred","darkblue"))+
  geom_smooth(color="red",data=filter(dfi,Type=="Negative abrupt"))+
  geom_smooth(color="royalblue",data=filter(dfi,Type=="Positive abrupt"))+
  xlab("\nFeature value")+
  ylab("SHAP value \n[impact on model output]\n")+
  facet_wrap(~variable,scales = "free",nrow = 1)+
  theme(legend.position = "none",strip.background = element_blank(),strip.text.x = element_blank())

PP2=grid.arrange(p1, p2, nrow = 2)
```